{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Exploratory Data Mining via Search Strategies Lab #5\"\nauthor: \"Ross Jacobucci & Kevin J. Grimm\"\nfontsize: 8pt\noutput:\n  beamer_presentation:\n    colortheme: \"wolverine\"\n    fonttheme: \"structurebold\"\n---\n## Outline\n\nThis presentation will go over the basics of using various multivariate procedures in R. These include:\n\n1. Exploratory Factor Analysis \\& PCA\n2. Structural Equation Models\n3. SEM Trees\n4. Other Multivariate CART Models\n\n\n## R Packages\n\nEFA \\& SEM\n```{r,message=FALSE}\nlibrary(psych) # efa & miscellaneous tools\nlibrary(OpenMx) # SEM\nlibrary(lavaan) # SEM\n```\n\nSEM Trees\n```{r,message=FALSE}\n# how to install\n#source('http://www.brandmaier.de/semtree/getsemtree.R')\nlibrary(semtree)\n```\n\nOther Longitudinal Trees\n\nlongRPart package. Not currently maintained on CRAN. Can be accessed from:\nhttp://cran.r-project.org/src/contrib/Archive/longRPart/\n\nFirst Download the 1.0 tar.gz\n```{r,message=FALSE}\n# longRPart is not on CRAN\n# have to install from source\nlibrary(longRPart)\nlibrary(REEMtree)\n```\n\n## PCA and EFA\n\nTo do PCA:\nprcomp() -- built-in\n\nTo do EFA:\nfactanal() -- builtin\n\nfa() -- from **psych**; multiple upgrades\n\nefaUnrotate() -- from **semTools**; can do FIML for missing data and WLSMV for categorical variables\n\nGPA() -- from **GPArotation** -- one stop shop for factor rotations\n\n**nFactors** package contains various functions for determining number of factors\n\nCFA:\ncfa() -- from **lavaan** package\n\nGeneral SEM:\n**OpenMx** -- can do cfa,sem, mixtures, differential equations...Most general package\n\n**lavaan** -- modeled after Mplus; can do maybe 80% of the things that Mplus can\n\nWe will be using the Holzinger Swineford dataset for all of the examples. Data from lavaan package\n\n```{r,message=FALSE}\nlibrary(lavaan)\nlibrary(OpenMx)\n# can't get OpenMx from CRAN\n#source('http://openmx.psyc.virginia.edu/getOpenMx.R')\nHS <- HolzingerSwineford1939\n#summary(HS)\n#str(HS)\n```\n\n# PCA, EFA, CFA\n\n## PCA\n\n```{r}\npca.out <- prcomp(HS[,7:15])\n#quartz()\nplot(pca.out)\n```\n\nSlightly ambiguous as to the number of components to retain, but we can see that the 3 components with eigenvalues above 1 (Kaiser rule). But in looking at the actual loadings, it almost looks like there is a general component, and maybe a couple specific components. \n\n## PCA continued\nThe psych package has a PCA function, principal(), which uses the same algorithm, but provides much more helpful output.\n\n```{r,message=FALSE}\nlibrary(psych)\nprin1 <- principal(HS[,7:15])\nloadings(prin1)\n```\n\n## 2 Components\n```{r}\nprin2 <- principal(HS[,7:15],2)\nloadings(prin2)\n```\n## 3 Components\n```{r}\nprin3 <- principal(HS[,7:15],3)\nloadings(prin3)\n\n```\n## 4 Components\n```{r}\nprin4 <- principal(HS[,7:15],4)\nloadings(prin4)\n```\n\n## PCA Continued\nNote, PCA always extracts the same number of components as variables entered. But with principal() we have a choice of displaying a specific number of components.\n\n\nIn using PCA, 3 components seems to be a little bit cleaner,where we can see \"clusters\" in the loadings, than others in the factor structure. But still hazy. With 4 components, the last component is really only made up of 1 variable (loading > 0.9). \n\n\nOne of the best tools that I know of to determine the number of components(PCA) or factors(EFA) is Horn's parallel analysis from the psych package.\n\nParallel analysis compares the actual eignevalues to the eigenvalues from a simulated dataset of random noise variables. We are looking for the number of eigenvalues above what would be expected by chance. This makes it look pretty clear, both 3 components and factors\n\n## Parallel Analysis\n\nAlthough called fa.parallel() it extracts both components and factors\n```{r}\nfa.parallel(HS[,7:15])\n```\n\n## Parallel Analysis With Items\n\n\nNot Run\n```{r,message=FALSE,eval=FALSE}\nlibrary(random.polychor.pa)\ndata(bfi)\nbfi.data<-na.exclude(as.matrix(bfi[1:200, 1:5]))\nout <- random.polychor.pa(nrep=3, data.matrix=bfi.data, q.eigen=.99)\n```\n\nIf your variables have 1-5 or 6 categories, then steps need to be taken to change the estimation procedure for a number of methods we will be talking about, including parallel analysis.\n\n## EFA\n\nR has the built-in factanal() which gets the job done in most cases. Defaults to ML estimation and varimax(orthogonal rotation)\n```{r}\nfa.out <- factanal(HS[,7:15],3);loads <- fa.out$loadings\nfa.out\n```\n\n## Rotation\n\n```{r}\n# cluster.plot(fa.out)\n# extract loadings and feed to rotation program.\nlibrary(GPArotation)\ngpa.out <- GPFoblq(loads) # oblique rotation\n# new loading matrix\nround(gpa.out$loadings,2)\n# new factor correlations\ngpa.out$Phi\n```\nFairly clear factor structure. Not many cross-loadings.\n\nFancy way to plot results, from http://mindingthebrain.blogspot.com/2015/04/plotting-factor-analysis-results.html\n\n## Factor Scores\nGet factor scores:\n```{r}\nfa.out2 <- fa(HS[,7:15],scores=\"Bartlett\")\nfscor <- fa.out2$scores\nhead(fscor)\n```\nNot generally advisable to get factor scores as there are a number of inherent problems with them (Grice 2001), but the psych package's fa() has multiple options. see \"scores=\" option.\n\n\n## Confirmatory Factor Analysis\n\nWhat if we have an idea as to what the structure of a model is?\n\n\nGreat tutorial: http://lavaan.ugent.be/tutorial/tutorial.pdf\n\n\nThis means we can specify a simple structure to the model\n\n```{r}\nHS.model <- ' visual =~ x1 + x2 + x3\n              textual =~ x4 + x5 + x6\n              speed =~ x7 + x8 + x9 '\nfit <- cfa(HS.model, data=HolzingerSwineford1939)\n#summary(fit, fit.measures=TRUE) too much output\ncoef(fit)\n\n```\n\n\n## Plot CFA Model\n\n```{r,fig.height=4.5}\nsemPlot::semPaths(fit,what=\"est\")\n```\n\n## Various SEM (CFA) Tools\nGet Fit Measures\n```{r,eval=FALSE}\nfitMeasures(fit)\n```\nModification Indices\n```{r,eval=FALSE}\nmodindices(fit)\n```\nNote that lavaan actually has four different functions to run models:\n\nlavaan() - requires you to specify the full model\n\ncfa() - only have to specify part of the model, makes assumptions for CFA models\n\nsem() - makes assumptions typical in more complex sem models\n\ngrowth() - makes specifying LGM easy, only need intercept and slope specification\n\n# Longitudinal Analyses\n\n## WISC Data\n\n\nFor this demonstration, we are going to compare the different packages and functions in analyzing longitudinal WISC data. Data is WISC4VPE.DAT.\n\n```{r}\nwisc <- read.table(\"C:/Users/RJacobucci/Documents/GitHub/EDM_Labs/2015/wisc4vpe.dat\")\nnames(wisc)<- c(\"V1\",\"V2\",\"V4\",\"V6\",\"P1\",\"P2\",\"P4\", \"P6\", \"Moeducat\")\n# note: V1 refers to verbal scores at grade 1, P is performance\n```\n\n\n## Visualization\n\nTo use many of the packages in R for longitudinal data (nlme), it is many times required to create a \"long\" data file, instead of the default wide.\n\nHow to do:\n```{r}\n# get rid of performance variables\nwisc.verb <- wisc[,c(1:4,9)]\n\n# create subset for plotting\nntot <- nrow(wisc.verb)    # total number of observations\nwisc.verb.sel <- wisc.verb[sample(ntot, 30), ]\n\nwisc.long <- reshape(wisc.verb, varying = c(\"V1\", \"V2\", \"V4\", \"V6\"), v.names = \"verbal\",\n                times = c(1, 2, 4, 6), direction = \"long\")\n\nwisc.long.sel <- reshape(wisc.verb.sel, varying = c(\"V1\", \"V2\", \"V4\", \"V6\"),\n                         v.names = \"verbal\", times = c(1, 2, 4, 6), \n                         direction = \"long\")\nhead(wisc.long,3)\nnames(wisc.long)[2] <- \"grade\"\nnames(wisc.long.sel)[2] <- \"grade\"\n\n```\n\n\n## Trajectories\n\nLets take a look at what the trajectories are:\n\nFirst using lattice package\n```{r,message=FALSE,fig.height=5.5}\nlibrary(lattice)\nxyplot(verbal ~ grade, groups = id, data = wisc.long, type = \"o\", col = \"black\", \n       xlab = \"Grade of Testing\", ylab = \"Verbal[t]\")\n```\n\n## Subset Trajectories\nThis is hard to see, better to use subset\n```{r,fig.height=5.5}\nxyplot(verbal ~ grade, groups = id, data = wisc.long.sel, type = \"o\", \n       col = \"black\", xlab = \"Grade of Testing\", ylab = \"Verbal[t]\")\n# on average, scores went up over time\n```\nThats a little better.\n\n## GGplot2 \n\nBut, can we simultaneously view the trajectories over time while seeing the influence that Mother's education may have?\n\n```{r,message=FALSE,fig.height=4.5}\nlibrary(ggplot2)\nqplot(grade,verbal,group=id,data=wisc.long,alpha=I(1/2),colour=factor(Moeducat),\n      geom = c(\"line\",\"point\"),xlab = \"Grade of Testing\", ylab = \"Verbal[t]\")\n```\nIt seems pretty clear that mothers with higher levels of education have children that are consistently higher in verbal performance across time.\n\nSo now that we have an idea what will we find if we look at the relationship between mother's education and trajectory, lets test it with statistical models\n\n# SEM Trees\n\n## SEM Trees LGCM\n\n### Note: SEM Trees can be used with any type of SEM that you can specify in lavaan or OpenMx\n\nOur first example is going to be using a latent growth curve model (lgcm) as our outcome, and attempting to find subgroups based on mother's education and the performance scores\n\nPrevious demonstrations using SEM Trees have used OpenMx. In this case, we will use lavaan.\n\n```{r,message=FALSE}\nlinearGCM <- '\n    inter =~ 1*V1 + 1*V2 + 1*V4 + 1*V6\n    slope =~ 1*V1 + 2*V2 + 4*V4 + 6*V6\n    inter ~~ vari*inter; inter ~ meani*1;\n    slope ~~ vars*slope; slope ~ means*1;\n    inter ~~ cov*slope;\n    V1 ~~ residual*V1; V1 ~ 0*1;\n    V2 ~~ residual*V2; V2 ~ 0*1;\n    V4 ~~ residual*V4; V4 ~ 0*1;\n    V6 ~~ residual*V6; V6 ~ 0*1;'\nrun <- lavaan(linearGCM,wisc) # could also have used growth()\n#summary(run)\ncoef(run)\n```\n\n\n## OpenMx\nSame LGM as specified in lavaan\n\n```{r,message=FALSE}\nresVars <- mxPath( from=c(\"V1\", \"V2\", \"V4\", \"V6\"), arrows=2,\n                 free=TRUE, values = c(1,1,1,1),\n                 labels=c(\"residual\",\"residual\",\"residual\",\"residual\") )\nlatVars<- mxPath( from=c(\"intercept\",\"slope\"), arrows=2, connect=\"unique.pairs\",\n                 free=TRUE, values=c(1,.4,1), labels=c(\"vari1\",\"cov1\",\"vars1\") )\nintLoads<- mxPath( from=\"intercept\", to=c(\"V1\", \"V2\", \"V4\", \"V6\"), arrows=1,\n             free=FALSE, values=c(1,1,1,1) )\nsloLoads<- mxPath( from=\"slope\", to=c(\"V1\", \"V2\", \"V4\", \"V6\"), arrows=1,\n               free=FALSE, values=c(1,2,4,6) )\nmanMeans<- mxPath( from=\"one\", to=c(\"V1\", \"V2\", \"V4\", \"V6\"), arrows=1,\n              free=FALSE, values=c(0,0,0,0) )\nlatMeans<- mxPath( from=\"one\", to=c(\"intercept\",\"slope\"), arrows=1,\n                free=TRUE,  values=c(0,1), labels=c(\"meani1\",\"means1\") )\ndataRaw<- mxData( observed=wisc[,c(1:4,9)], type=\"raw\" )\nlgm.mod<- mxModel(\"LGM\", type=\"RAM\",\n                manifestVars=c(\"V1\", \"V2\", \"V4\", \"V6\"),\n                latentVars=c(\"intercept\",\"slope\"),dataRaw,\n                resVars, latVars, intLoads, sloLoads, manMeans, latMeans)\nmod.run <- mxRun(lgm.mod);coef(mod.run)\n```\n\n\n## Run semtree()\nNow use \"run\" with semtree()\n\n### Note: semtree() works better with OpenMx at this time\n\nJust used defaults\n```{r,message=FALSE,fig.height=5}\nmytree <- semtree(run,wisc[,c(1:4,9)]) # only moeducat as covariate\nplot(mytree)\n```\n\n## Plot Trajectories\n\n```{r,fig.height=5}\n# create expected trajectories from parameters\nexpected.growth <- matrix(\n    rep(t(parameters(mytree))[, \"meani\"], each=4) +\n    rep(t(parameters(mytree))[, \"means\"], each=4)*c(1,2,4,6), nrow=2, byrow=T)\n# plot expected trajectories for each leaf\nplot(c(1,6), c(10,50), xlab=\"Grade\", ylab=\"Verbal Score\", type=\"n\",main=\"SEM Trees LGCM\")\nlines(c(1,2,4,6), expected.growth[1,], col=\"red\", type=\"b\", lw=3)\nlines(c(1,2,4,6), expected.growth[2,], col=\"blue\", type=\"b\", lw=3)\nlegend(\"bottomright\", c(\"Mom Ed. = 0\", \"Mom Ed. = 1 or 2\"),col=c(\"red\",\"blue\"), lw=3)\n```\n\n\n## SEM Trees Results Continued\nWe should get same results as in the left node of the tree by just subsetting the dataset based on Moeducat = 0\n\nould get same results as in the left node of the tree by just subsetting the dataset based on Moeducat = 0\n\n```{r}\nwisc.sub <- wisc[wisc$Moeducat == 0,]\nrun.sub <- lavaan(linearGCM,wisc.sub)\ncoef(run.sub)\n```\nYup, everything checks out. This should make it clear that SEM Trees is really just subsetting the dataset into subgroups based on values of the covariates entered.\n\n# SEM Trees Options\n\n## Invariance of parameters\n```{r,eval=FALSE}\nmodel <- ' i =~ 1*t1 + 1*t2 + 1*t3 + 1*t4\n           s =~ 0*t1 + l1*t2 + l2*t3 + 3*t4 '\nfit <- growth(model, data=Demo.growth)\ntree.inv = semtree(fit,Demo.growth[,1:5],invariance=c(\"l1\",\"l2\"))\n```\n\nThis allows parameters to be freely estimated, but forces them to be the same in each group tested\n\n## Additional Options\n\n```{r,message=FALSE}\n#?semtree.control\ncontrol <- semtree.control(method=\"fair\",min.N=20,bonferroni=TRUE)\ntree2 = semtree(mod.run,wisc[,c(1:4,9)],control=control)\n#plot(tree2) Doesn't change results\n```\nI prefer to set the method=\"fair\" as the default \"naive\" exhibits a preference for covariates with a large number of response options.\n\\\n\\\nAdditionally, I usually set the minimum number per node to be something greater than 20. Remember, each node is an actual SEM model.\n\\\n\\\nFinally, if there are a large number of covariates, setting bonferroni=TRUE corrects for the number of comparisons. Usually doesn't make a difference\n\n# SEM Trees 2nd Example\n\n\n## Lavaan Syntax\n\nFor the 2nd example we will use a mediation model specified in lavaan \n\n```{r}\n# latent variable definition           =~ is measured by\n# regression                           ~ is regressed on\n# (residual) (co)variance             ~~ is correlated (covariance) with\n# intercept (mean)                  ~ 1 intercept   # same as regressed, but with 1\n# mediation parameter definition   :=\n```\n\n## Mediation Example\n\nBased on example from:\nhttp://lavaan.ugent.be/tutorial/mediation.html\n\n```{r}\nset.seed(1234)\nX <- rnorm(100)\nclass <- rbinom(100,1,0.5)\nM <- class*0.5*X + rnorm(100)\nY <- class*0.7*M + rnorm(100)\nData <- data.frame(X = X, Y = Y, M = M,class=class)\nmodel <- ' # direct effect\n             Y ~ c*X\n           # mediator\n             M ~ a*X\n             Y ~ b*M\n           # indirect effect (a*b)\n             ab := a*b\n           # total effect\n             total := c + (a*b)\n         '\nfit.med <- sem(model, data = Data)\n```\n\n## Visualize the Model\n\n```{r}\nsemPlot::semPaths(fit.med)\n```\n\n## Mediation Continued\n```{r}\nsummary(fit.med)\n```\n\n## SEM Trees and Mediation\n\n```{r,message=FALSE}\ntree.med <- semtree(fit.med,Data)\nplot(tree.med)\n```\n\n## SEM Trees Conclusion\n\nIn the mediation example, SEM Trees correctly identified the groups with vastly different parameter estimates. \n\n\nThe semtree package is still under development and bugs pop up occasionally. As an example, semtree currently doesn't work with R 3.3.1, thus why we had you install 3.2.5. \n\n\n### Important Reminder: the models you can run with SEM Trees is not limited to the type of models we presented. Any type of SEM can be used to search for groups.\n\n# Mixed Effects Trees\n\n## longRPart\n\nInstead of running the models in a SEM framework, longRPart uses mixed-effects models. This works just as well, as many LGCM can be re-specified as mixed-effects models.\n\nFor this we will use the nlme package to run a linear mixed effects model\n\n## nlme\n\n```{r}\n#Linear growth\nmix1 <- lme(fixed = verbal ~ grade, random = ~ grade | id, \n            data = wisc.long, method=\"ML\" )\nsummary(mix1) # get same estimates as in LGM, notice SD not VAR\n```\n\nNow that we see how we can specify growth curves as mixed-effects models, lets test out w/ longRPart and see if we get the same answer to SEM Trees\n\n## longRPart\n\n```{r,message=FALSE,results='hide'}\nlcart.mod1 <- longRPart(verbal ~ grade,~ Moeducat, ~ 1 | id,wisc.long)\n```\n```{r}\nsummary(lcart.mod1)\n```\n\n## Plot Tree\n```{r,fig.height=5}\nlrpPlot(lcart.mod1)\nlrpTreePlot(lcart.mod1,use.n=F)\n```\n\nGet almost identical results as from SEM Trees, but it looks as though longRPart allows more flexibility in the slopes between time points (grade).\n\n\n\n## Additional Multivariate Trees\n\n\n### REEMtree\n\nTree partitioning for longitudinal data where random effects exist. This doesn't really accomplish what we did previously with longRPart or SEM Trees. Interested, see the examples in following links.\n\nhttp://pages.stern.nyu.edu/~jsimonof/REEMtree/\n\nhttp://www.r-bloggers.com/a-brief-tour-of-the-trees-and-forests/\n\n### mvpart\n\nLike longRPart, also archived:\n\nhttp://cran.r-project.org/src/contrib/Archive/mvpart/\n\nIf we treat the longitudinal data just as a multivariate outcome, we can accomplish a very similar process.\n\n### mvtboost\n\nMultivariate boosting\n\nSee for example code:\nhttps://github.com/patr1ckm/mvtboost\n",
    "created" : 1481566540601.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4000067949",
    "id" : "1691C99B",
    "lastKnownWriteTime" : 1471529468,
    "last_content_update" : 1471529468,
    "path" : "~/GitHub/SearchWkshp_labs16/lab5.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}